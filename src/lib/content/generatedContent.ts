// Auto-generated content imports - do not edit manually
// Generated by scripts/generateContentImports.ts
// This file provides type-safe access to all blog content at build time

import type { SerializedPost } from '@/types/post'

import css_fix_social_icon_flicker_on_theme_toggle_md_content from '@/content/blog/css/fix-social-icon-flicker-on-theme-toggle.md?raw'
import inkdrop_how_i_learn_new_framework_md_content from '@/content/blog/inkdrop/how-i-learn-new-framework.md?raw'
import inkdrop_notes_in_service_of_doing_choosing_inkdrop_md_content from '@/content/blog/inkdrop/notes-in-service-of-doing-choosing-inkdrop.md?raw'
import neovim_native_lsp_setup_in_neovim_0_11__on_macos_md_content from '@/content/blog/neovim/native-lsp-setup-in-neovim-0.11+-on-macos.md?raw'
import tools_build_time_content_generation_md_content from '@/content/blog/tools/build-time-content-generation.md?raw'
import typescript_learning_typescript_through_constraint_not_tutorials_md_content from '@/content/blog/typescript/learning-typescript-through-constraint-not-tutorials.md?raw'
import web_development_implementing_dynamic_favicons_md_content from '@/content/blog/web-development/implementing-dynamic-favicons.md?raw'
import web_development_pnpm_mono_repo_setup_md_content from '@/content/blog/web-development/pnpm-mono-repo-setup.md?raw'
import web_development_security_first_markdown_processing_md_content from '@/content/blog/web-development/security-first-markdown-processing.md?raw'
import wezterm_wezterm_terminal_setup_md_content from '@/content/blog/wezterm/wezterm-terminal-setup.md?raw'

// Raw markdown content accessible by file path
export const contentModules = {
  '@/content/blog/css/fix-social-icon-flicker-on-theme-toggle.md': css_fix_social_icon_flicker_on_theme_toggle_md_content,
  '@/content/blog/inkdrop/how-i-learn-new-framework.md': inkdrop_how_i_learn_new_framework_md_content,
  '@/content/blog/inkdrop/notes-in-service-of-doing-choosing-inkdrop.md': inkdrop_notes_in_service_of_doing_choosing_inkdrop_md_content,
  '@/content/blog/neovim/native-lsp-setup-in-neovim-0.11+-on-macos.md': neovim_native_lsp_setup_in_neovim_0_11__on_macos_md_content,
  '@/content/blog/tools/build-time-content-generation.md': tools_build_time_content_generation_md_content,
  '@/content/blog/typescript/learning-typescript-through-constraint-not-tutorials.md': typescript_learning_typescript_through_constraint_not_tutorials_md_content,
  '@/content/blog/web-development/implementing-dynamic-favicons.md': web_development_implementing_dynamic_favicons_md_content,
  '@/content/blog/web-development/pnpm-mono-repo-setup.md': web_development_pnpm_mono_repo_setup_md_content,
  '@/content/blog/web-development/security-first-markdown-processing.md': web_development_security_first_markdown_processing_md_content,
  '@/content/blog/wezterm/wezterm-terminal-setup.md': wezterm_wezterm_terminal_setup_md_content
}

// Processed blog posts with metadata and content
// Note: dates are serialized as ISO strings and must be converted to Date objects
export const processedPosts: SerializedPost[] = [
  {
    "title": "Why transition: all Is Risky (CSS Transition Pitfall)",
    "date": "2025-12-04T00:00:00.000Z",
    "description": "It wasn’t catastrophic but once you see it, you can’t unsee it. On slower machines or reduced motion environments, it became even more noticeable.",
    "tags": [
      "css",
      "bugs",
      "debugging",
      "fix"
    ],
    "slug": "fix-social-icon-flicker-on-theme-toggle",
    "topic": "css",
    "content": "\n# The Symptom\n\nWhile working on a dark mode toggle, I noticed something subtle but irritating.\nThe social icons would briefly flash before snapping into their correct color\nwhile the rest of the UI transitioned smoothly. At first glance, it looked like\na hydration issue, a delayed theme state update, or even an SVG rendering problem.\nBut after digging deeper, it turned out to be something much simpler.\n\n## The Root Cause\n\nSurprisingly, the issue wasn’t React, the theme toggle, or Vite. I spent way too\nlong checking React DevTools before I thought to look at the CSS. The culprit was\na single line of CSS:\n\n```css\ntransition: all;\n```\n\nBecause the icons used `filter` for coloring, specifying `transition: all` caused\nthe browser to animate every possible property. During the theme change, some of\nthese properties temporarily passed through invalid or intermediate visual states,\nwhich is exactly what caused the flicker.\n\n## The FIX\n\nThe solution was simple once I understood the problem. Instead of transitioning\nevery property, I limited the transition to exactly what I needed. Updating the\nCSS for the social icons to:\n\n```css\n.socials__icon {\n  /* Transition only filter to prevent unwanted flickers */\n  transition: filter var(--transition-duration-normal);\n  filter: drop-shadow(0 0 0 transparent);\n}\n```\n\nOnce I changed:\n\n```css\ntransition: all;\n```\n\nto:\n\n```css\ntransition: filter;\n```\n\nThe flicker disappeared entirely.\n\nNo JavaScript changes.\nNo theme logic changes.\nJust a scoped transition.\n\n# A Lesson in transition: all\n\n> `transition: all` is convenient — but often dangerous.\n\nThis experience reinforced an easy-to-forget rule. Animating all properties can\ncause unintended visual glitches during state changes, trigger strange\nintermediate render states, and make bugs harder to reason about. Explicitly\ntargeting only the properties that need animation is safer, more predictable, and\nprevents subtle UI regressions.\n\nThis small bug reminded me that some of the most valuable lessons come from tiny,\nalmost invisible issues. It wasn’t about performance, complex architecture, or\nadvanced animations. It was about paying attention to the details. I'd been\ncareless with `transition: all` because it was quick and seemed harmless, but\nthat one line caused a subtle visual glitch that took way too long to track down.\n\n## Key Takeaway\n\nBe intentional with transitions. Target only the properties you need to animate\nand avoid `transition: all` unless you really mean it. This prevents subtle\nvisual glitches and makes your UI more predictable.\n",
    "readingTime": 2
  },
  {
    "title": "How I Learn New Framework",
    "date": "2025-12-22T00:00:00.000Z",
    "description": "As a full time Solutions Consultant and self-taught developer, one of the hardest challenges is keeping pace with the ever-changing world of web development while still grounding myself in the fundamentals.",
    "tags": [
      "learning",
      "productivity",
      "personal growth",
      "developer journey"
    ],
    "slug": "how-i-learn-new-framework",
    "topic": "inkdrop",
    "content": "\n## What Drives Me\n\nEvery time a new framework emerges, it shines like a freshly polished tool in a crowded workshop. Its simplicity, elegance, and promise of quick mastery can easily lure beginners like me. I’ve felt that pull, the temptation to dive in headfirst simply because it looks easy to use.\n\nBut I’ve learned to pause. Instead of chasing novelty for its own sake, I ask myself: What tangible benefits does this bring? Which problems does it truly solve compared to my current workflow? Why choose this path over another? And—most importantly—when does it make sense for the project I’m building?\n\nThis approach turns learning from a superficial sprint into a deliberate, meaningful journey. It’s less about following trends and more about understanding purpose.\n\n## Learning the Patterns\n\nWhen I dive into a new framework, one of the first things I notice is the syntax—the patterns, the way pieces fit together. As a beginner, logic often feels like a foreign language. I’ve learned that it’s okay to feel lost at the start. That confusion, frustrating as it can be, is also the part that makes the journey thrilling: the puzzle of picking up tiny fragments of information and slowly connecting them into something meaningful.\n\nI’ve accepted that there’s no shortcut. When I get stuck, I turn to ChatGPT, asking it to explain things in beginner-friendly ways, often with analogies I can relate to. I don’t shy away from the “dumb” questions: What is a React hook? Why do some YouTubers use Zustand? Can I skip React hooks and jump straight to Zustand?\n\nSure, my questions reveal my confusion—but over days, the pieces start to click. I begin to see why Zustand is not a replacement but an essential tool, and why Zustand becomes valuable when managing complex state, like comments and reactions on my blog. Simple tasks like theming still suit useState, but when coordination grows tricky, a tool like Zustand turns chaos into clarity. Slowly, I begin to understand what “complicated” really means in the context of code—and why learning the basics matters before chasing the next shiny tool.\n\n## We Humans Are Not Perfect\n\nAnother crucial part of my learning journey is embracing imperfection. I’ve learned that it’s not enough to simply jot down solutions—I need to capture the confusion, the “why” behind each step, and the thoughts that swirl around my head as I try to make sense of it all.\n\nThis is where [Inkdrop](https://www.inkdrop.app/) comes in. It’s a note-taking app that doesn’t distract me, doesn’t pull me away from the flow of learning. Instead, it gives me focus. It allows me to write freely, to explore my thoughts, and to slowly connect the dots at my own pace. Less friction, more space for curiosity—and more time to truly understand.\n\n## The Journey Matters More Than the Tools\n\nAt the end of the day, it’s not about the newest framework, the slickest library, or the trendiest tool. It’s about the process—the curiosity, the mistakes, the small victories, and the slow, steady understanding that comes from embracing the unknown.\n\nAs a full time Solutions Consultant and self-taught developer, every confusion is a teacher, every “why” is a doorway, and every note I take is a map I can revisit later. The frameworks will change, the syntax will evolve, but the way I learn, reflect, and connect the dots—that is what lasts.\n\nFrameworks change fast. Learning how you learn is the part that compounds.\n",
    "readingTime": 3
  },
  {
    "title": "Notes in Service of Doing: Choosing Inkdrop",
    "date": "2025-12-21T00:00:00.000Z",
    "description": "In this post, I want to share why I chose Inkdrop over tools like Obsidian and Notion—not because they fall short, but because of how I work as a solutions consultant and a part‑time web developer.",
    "tags": [
      "inkdrop",
      "note-taking",
      "developers",
      "productivity",
      "knowledge-management",
      "tools"
    ],
    "slug": "notes-in-service-of-doing-choosing-inkdrop",
    "topic": "inkdrop",
    "content": "\n## What is [Inkdrop](https://www.inkdrop.app/)?\n\nOn the surface, [Inkdrop](https://www.inkdrop.app/) is a note‑taking app for developers. You write in Markdown, you organize your thoughts, and you move on. But for me, it’s more than a notes app—it’s a deliberate constraint, and that’s exactly why it works.\n\n## Simple, Yet Powerful\n\nMost note‑taking apps I’ve tried are incredibly powerful. They come with endless features, deep customization, and thriving plugin ecosystems. Ironically, that power is also their biggest weakness—for me.\n\nWith so many knobs to turn and settings to tweak, I found myself spending more time _designing my note‑taking system_ than actually thinking or working. It became just another rabbit hole, layered on top of my split keyboard obsession and my ever‑evolving Neovim configuration. Another abstraction. Another system to maintain.\n\n[Inkdrop](https://www.inkdrop.app/) isn’t immune to this—you _can_ customize it—but its limits are clearly defined. And that’s what I love about it. The options are intentionally constrained, just enough to get you started quickly and focused on writing. Instead of spending five hours testing hundreds of plugins in search of a “perfect” setup that never really exists, I can sit down and start working through my thoughts.\n\n## When Ideas Become the Product\n\nOne of the most appealing features of modern note‑taking tools is the ability to connect ideas—to visualize them, link them, and grow a so‑called _second brain_. I get the appeal. I genuinely love ideas, and I love seeing how they relate to one another.\n\nBut I’ve also seen how easy it is to get lost in that process.\n\nI’ve watched countless tutorials where the focus shifts from thinking clearly to maintaining an elaborate web of connections. The visual becomes the goal, not the thinking behind it. Over time, the notes stop supporting the work and quietly _become the work_.\n\n## Execution Over Elegance\n\nAs a full‑time solutions consultant and part‑time developer, my job is not to build the most beautiful knowledge system imaginable. My job is to execute. Ideas matter—but they are a means, not the end.\n\nI’ve personally experienced how easy it is to become excellent at organizing notes while struggling to move ideas into reality. Time spent refining note architecture is time not spent designing real systems, solving real problems, or shipping real solutions.\n\nThis isn’t a criticism of Obsidian or Notion. They are fantastic tools, and for many people, they’re the right choice. For me, the deciding factor is cognitive overhead. [Inkdrop](https://www.inkdrop.app/) stays out of my way. It helps me think just enough—and then nudges me back to doing.\n\nAnd right now, that balance is exactly what I need.\n\n## The Community\n\nOne thing that surprised me most about [Inkdrop](https://www.inkdrop.app/) wasn’t the app itself, but the people around it.\n\nCompared to larger tools, the [Inkdrop](https://www.inkdrop.app/) community is small—and to me, that’s a feature, not a limitation. In the [Inkdrop Discord](https://my.inkdrop.app/login?redirect=/discord), the scale is just right: small enough that names feel familiar, conversations carry over from one day to the next, and people don’t disappear into noise.\n\nIt doesn’t feel like standing in a crowded room talking to strangers. It feels like a community.\n\nWhen you ask a question, you’re not shouting into the void. You get thoughtful responses, learn from others workflows, and sometimes just exchange ideas without the pressure to perform or optimize. There’s a sense of connection that’s hard to find in larger ecosystems.\n\nWhat surprised me even more is that while the space is centered around [Inkdrop](https://www.inkdrop.app/), the conversations often go far beyond it. You’ll see discussions about Neovim workflows, React patterns, or how to build [Inkdrop](https://www.inkdrop.app/) plugins—deep, thoughtful responses that are hard to come by in much larger Discord servers, where signal is often drowned out by scale.\n\nIn a space built around thinking clearly and working deliberately, this kind of human‑scale community feels like a natural extension of the tool itself.\n\n## Choosing to Commit\n\n[Inkdrop](https://www.inkdrop.app/) isn’t free. There’s a 30‑day trial—quiet, unhurried, and long enough to tell you whether this tool fits the way you think. By the time the month ends, you don’t need a spreadsheet to decide. You already know.\n\nBut trying [Inkdrop](https://www.inkdrop.app/) isn’t just about testing a product. It’s an invitation to slow the noise, to work with fewer knobs to turn, and to pay attention to what actually matters. If you decide to step in, don’t do it alone—join the conversation. Linger in the community. Ask questions. Listen.\n\nThat’s when the price reveals its real shape. Not as a fee, but as a commitment—to focus over friction, to execution over endless preparation, and to a small, thoughtful space where work gets done quietly and well.\n",
    "readingTime": 4
  },
  {
    "title": "macOS Neovim 0.11+ Native LSP Configuration",
    "date": "2025-12-20T00:00:00.000Z",
    "description": "Step by step guide on how to setup LSP in Neovim 0.11+ in macOS.",
    "tags": [
      "vim",
      "neovim",
      "lsp",
      "configuration"
    ],
    "slug": "native-lsp-setup-in-neovim-0.11+-on-macos",
    "topic": "neovim",
    "content": "\nNeovim 0.11 (released March 2025) introduced a simpler, fully native way to configure the Language Server Protocol (LSP). With this release, LSP becomes a true first-class citizen—no extra plugins required for basic setup.\n\nYou can still use Mason, a Neovim plugin that acts as a portable package manager for external development tooling such as LSP servers, DAP servers, linters, and formatters.\n\n> NOTE: Mason itself does not configure the LSP servers for use in Neovim. It just installs and manages the binaries/tools. To wire them up with Neovim built-in LSP client, you can use mason-lspconfig.nvim together with nvim-lspconfig.\n\nWhat we’ll cover today is a more manual approach. It requires installing LSP servers—such as lua-language-server—directly on your machine. This approach isn’t portable. You’ll need to reinstall the server when setting up a new machine, or automate it with a shell [script](https://github.com/rjleyva/dotfiles-macos/blob/main/scripts/dev-setup.sh).\n\nLet's begin with installing `lua-language-server` via homebrew:\n\n```bash\nbrew install lua-language-server\n```\n\n> NOTE: You can also install this via npm but I prefer to use homebrew for this instance.\n\n### Structure Overview\n\nThe structure is based on [Marco Peluso](https://www.youtube.com/watch?v=tdhxpn1XdjQ) YouTube video.\n\n```bash\nnvim/\n├── init.lua\n├── lsp/\n│   └── lua_ls.lua\n└── lua/\n    └── core/\n        └── lsp.lua\n```\n\n`lsp/` contains server specifications only, while `lua/core/` is responsible for enabling and orchestrating them. This keeps configuration declarative and avoids coupling server definitions to startup logic.\n\n### Step-by-Step Setup\n\nLet’s start by creating the `nvim` directory:\n\n```bash\nmkdir -p ~/.config/nvim\n```\n\nThen move to the `nvim` directory by running this command:\n\n```bash\ncd ~/.config/nvim\n```\n\nCreate the main `init.lua` file:\n\n```bash\nnvim init.lua\n```\n\nand add this configuration:\n\n```lua\nrequire('core.lsp')\n```\n\nNow let's create the `lsp` directory:\n\n```bash\nmkdir lsp\n```\n\nThen move to `lsp` directory by running this command:\n\n```bash\ncd lsp\n```\n\nCreate `lua_ls.lua` by running this command:\n\n```bash\nnvim lua_ls.lua\n```\n\nThen add this to `lua_ls.lua`:\n\n```lua\nlocal M = {}\n\nM.spec = {\n  cmd = {\n    'lua-language-server',\n  },\n\n  filetypes = {\n    'lua',\n  },\n\n  root_markers = {\n    '.git',\n    '.luacheckrc',\n    '.luarc.json',\n    '.luarc.jsonc',\n    '.stylua.toml',\n    'selene.toml',\n    'selene.yml',\n  },\n\n  settings = {\n    Lua = {\n      runtime = {\n        version = 'LuaJIT',\n      },\n      diagnostics = {\n        globals = { 'vim' },\n      },\n      hint = {\n        enable = true,\n        setType = true,\n        paramType = true,\n        -- paramName = 'All',\n        -- semicolon = 'All',\n        -- arrayIndex = 'All',\n        -- moduleName = 'All',\n      },\n      telemetry = {\n        enable = false,\n      },\n      workspace = {\n        checkThirdParty = false,\n        library = {},\n        -- Enable if you want:\n        -- Full API docs and completion for plugin development\n        -- Autocompletion for all vim.api.* functions\n        -- Uncomment the line below to index Neovim’s runtime and plugins:\n        -- library = vim.api.nvim_get_runtime_file(),\n      },\n    },\n  },\n\n  single_file_support = true,\n  autostart = false, -- manually enabled via vim.lsp.enable\n  log_level = vim.lsp.protocol.MessageType.Warning,\n}\n\nM.name = 'lua_ls'\n\nreturn M.spec\n```\n\n> NOTE: These markers define how a project root is detected. This becomes useful once you start managing multiple language servers consistently.\n\nGo back to `nvim` directory by running this command:\n\n```bash\ncd ~/.config/nvim\n```\n\nThen create `lua` and `core` directory:\n\n```bash\nmkdir -p lua/core/\n```\n\nThen move inside `core` directory:\n\n```bash\ncd lua/core/\n```\n\nand create `lsp.lua`:\n\n```bash\nnvim lsp.lua\n```\n\nThen enable it using `vim.lsp.enable` like this:\n\nThis explicitly enables the Lua language server by name:\n\n```lua\nvim.lsp.enable({\n  'lua_ls',\n})\n```\n\n> NOTE: `vim.lsp.enable()` is available starting in Neovim 0.11 and replaces the need for `nvim-lspconfig` in simple setups.\n> This approach works well for most setups, but more complex workflows may still benefit from `lspconfig` or Mason integrations.\n\nInside Neovim you can run:\n\n```\ncheckhealth lsp\n```\n\nand you'll see something like this:\n\n```\nvim.lsp: Active Clients ~\n- lua_ls (id: 1)\n  - Version: 3.15.0\n  - Root directory: ~/dotfiles-macos\n  - Command: { \"lua-language-server\" }\n  - Settings: {\n      Lua = {\n        diagnostics = {\n          globals = { \"vim\" }\n        },\n        hint = {\n          enable = true,\n          paramType = true,\n          setType = true\n        },\n        runtime = {\n          version = \"LuaJIT\"\n        },\n        telemetry = {\n          enable = false\n        },\n        workspace = {\n          checkThirdParty = false,\n          library = {}\n        }\n      }\n    }\n  - Attached buffers: 2, 3\n```\n\nThis output confirms that `lua_ls` is running successfully.\n\nThis setup is what I currently use daily in my Neovim workflow.\n\n### Why use native LSP in Neovim 0.11+?\n\nMy main reason for choosing this approach is simple: fewer plugins to maintain, faster startup times, and easier debugging. While manually configuring LSP takes a bit more effort, it taught me a lot about what’s actually happening behind the scenes. I’m in control of everything, and each configuration exists because I need it—not because a plugin decided for me.\n\n### Who is this for?\n\nThis setup is ideal if you:\n\n- Use Neovim 0.11+\n- Want fewer plugins and more control\n- Prefer understanding how LSP works under the hood\n\nIf you want a fully portable, zero-setup experience, Mason may still be a better fit.\n\nThis approach scales naturally as you add more language servers. In future posts, I’ll cover keymaps, diagnostics, formatting, and multi-server setups—still using Neovim’s native LSP.\n\nYou can find the full working Neovim configuration [here](https://github.com/rjleyva/dotfiles-macos).\n\n### Next steps\n\n- Add more language servers using the same `lsp/` pattern\n- Define LSP keymaps and diagnostics\n- Integrate formatting without external plugins\n\nSee you in the next post.\n",
    "readingTime": 5
  },
  {
    "title": "Build-Time Content Generation",
    "date": "2025-12-24T00:00:00.000Z",
    "description": "A deep dive into my custom build-time content processing system that transforms markdown files into type-safe TypeScript modules, including the trade-offs and design decisions.",
    "tags": [
      "build-tools",
      "typescript",
      "markdown",
      "performance",
      "developer-experience",
      "content-management"
    ],
    "slug": "build-time-content-generation",
    "topic": "tools",
    "content": "\n# How I Process Blog Posts at Compile Time\n\nWhen I started building my personal blog, I wanted something that felt both fast and maintainable. Most static site generators handle content at build time, but I found myself wanting more control over the process. Instead of using a framework's content layer, I built my own build-time content generation system.\n\nIt started simple—a script to read markdown files and generate some TypeScript—but evolved into a sophisticated pipeline that validates content, calculates reading times, and creates type-safe imports. The result is a blog that loads instantly while giving me complete control over how content flows from markdown to the browser.\n\n## The Problem I Was Solving\n\nTraditional static site generators like Next.js or Astro handle content processing automatically. But I wanted:\n\n- **Type safety**: Content should be validated at build time, not runtime\n- **Performance**: No content parsing during page loads\n- **Flexibility**: Full control over how markdown gets transformed\n- **Developer experience**: Clear error messages and validation\n\nMy blog posts live as markdown files with YAML frontmatter:\n\n```yaml\n---\ntitle: 'Build-Time Content Generation'\ndescription: 'A deep dive into my custom build system'\ndate: '2025-12-24'\ntags: ['typescript', 'build-tools']\n---\n```\n\nThe challenge was transforming these files into something my React components could use efficiently.\n\n## The Solution: Build-Time Processing\n\nInstead of parsing markdown at runtime, I created a build script that runs during compilation. Here's how it works:\n\n### File Discovery\n\nThe system recursively scans my content directory, finding all `.md` files:\n\n```typescript\nconst discoverMarkdownFiles = (): string[] => {\n  const discoveredFiles: string[] = []\n\n  const scanDirectoryRecursively = (\n    absoluteDirectoryPath: string,\n    relativePathFromContentRoot = ''\n  ): void => {\n    const directoryEntries = fs.readdirSync(absoluteDirectoryPath, {\n      withFileTypes: true\n    })\n\n    for (const entry of directoryEntries) {\n      const fullEntryPath = path.join(absoluteDirectoryPath, entry.name)\n      const relativeEntryPath = path.join(\n        relativePathFromContentRoot,\n        entry.name\n      )\n\n      if (entry.isDirectory()) {\n        scanDirectoryRecursively(fullEntryPath, relativeEntryPath)\n      } else if (entry.isFile() && entry.name.endsWith('.md')) {\n        discoveredFiles.push(relativeEntryPath)\n      }\n    }\n  }\n\n  scanDirectoryRecursively(BLOG_CONTENT_DIRECTORY)\n  return discoveredFiles\n}\n```\n\nThis creates a flat list of all markdown files, preserving their relative paths.\n\n### Frontmatter Validation\n\nEach file gets parsed for YAML frontmatter with strict validation:\n\n```typescript\nconst validateFrontmatter = (\n  frontmatter: Partial<PostFrontmatter> | null | undefined,\n  filePath: string\n): PostFrontmatter => {\n  if (!frontmatter) {\n    throw new FrontmatterValidationError(\n      'No frontmatter found. Please add YAML frontmatter with required fields (title, date, description)',\n      filePath\n    )\n  }\n\n  if (\n    typeof frontmatter.title !== 'string' ||\n    frontmatter.title.trim() === ''\n  ) {\n    throw new FrontmatterValidationError(\n      'Missing or invalid \"title\" field. Title must be a non-empty string',\n      filePath\n    )\n  }\n\n  // ... date and description validation continues\n}\n```\n\nI built this validation because I wanted to catch content errors during builds, not in production. Every post must have a title, date, and description.\n\n### Content Processing\n\nFor each valid file, the system extracts metadata and processes the content:\n\n```typescript\nconst blogPost = {\n  title: parsedFile.frontmatter.title,\n  date: dateValue.toISOString(), // Serialized for JSON compatibility\n  description: parsedFile.frontmatter.description,\n  tags: parsedFile.frontmatter.tags ?? [],\n  slug: slugFromFilename,\n  topic: topicFromPath, // Extracted from directory structure\n  content: parsedFile.markdownBody,\n  readingTime: estimatedReadingTimeMinutes\n}\n```\n\nThe reading time calculation uses a simple but effective formula:\n\n```typescript\nconst wordCount = parsedFile.markdownBody\n  .split(/\\s+/)\n  .filter(word => word.trim().length > 0).length\n\nconst estimatedReadingTimeMinutes =\n  wordCount === 0 ? 0 : Math.ceil(wordCount / 200) // 200 WPM\n```\n\n### Type-Safe Module Generation\n\nThe most interesting part is how this generates actual TypeScript code:\n\n```typescript\nconst generateContentLoaderModule = (markdownFilePaths: string[]): string => {\n  const importStatements: string[] = []\n  const moduleMappings: string[] = []\n  const processedBlogPosts: SerializedPost[] = []\n\n  for (const relativeFilePath of markdownFilePaths) {\n    const importVariableName =\n      relativeFilePath.replace(/[^a-zA-Z0-9]/g, '_') + '_content'\n\n    const viteImportPath = `@/content/blog/${relativeFilePath}?raw`\n\n    importStatements.push(\n      `import ${importVariableName} from '${viteImportPath}'`\n    )\n\n    moduleMappings.push(\n      `  '@/content/blog/${relativeFilePath}': ${importVariableName}`\n    )\n  }\n\n  // Generate the complete module...\n}\n```\n\nThis creates a file that looks like:\n\n```typescript\n// Auto-generated content imports - do not edit manually\nimport css_fix_social_icon_flicker_md_content from '@/content/blog/css/fix-social-icon-flicker-on-theme-toggle.md?raw'\nimport typescript_learning_typescript_md_content from '@/content/blog/typescript/learning-typescript-through-constraint-not-tutorials.md?raw'\nimport type { SerializedPost } from '@/types/post'\n\nexport const contentModules = {\n  '@/content/blog/css/fix-social-icon-flicker-on-theme-toggle.md':\n    css_fix_social_icon_flicker_md_content,\n  '@/content/blog/typescript/learning-typescript-through-constraint-not-tutorials.md':\n    typescript_learning_typescript_md_content\n}\n\nexport const processedPosts: SerializedPost[] = [\n  {\n    title: 'Fix Social Icon Flicker on Theme Toggle',\n    date: '2025-12-04T00:00:00.000Z',\n    description: 'How I fixed a subtle CSS transition bug...'\n    // ... complete post metadata\n  }\n] as const\n```\n\n### Runtime Usage\n\nAt runtime, the content is immediately available:\n\n```typescript\nimport { processedPosts } from './generatedContent'\n\nconst convertProcessedPosts = (): Post[] => {\n  return processedPosts.map(post => ({\n    ...post,\n    date: new Date(post.date), // Convert back to Date objects\n    tags: [...post.tags]\n  }))\n}\n\nexport const posts: Post[] = convertProcessedPosts()\n```\n\n## Why This Approach?\n\n### The Benefits\n\n**Performance**: Content loads instantly because it's processed at build time. No runtime parsing means faster page loads and better SEO.\n\n**Type Safety**: TypeScript validates content structure during compilation. If I forget a required field, the build fails with a clear error message.\n\n**Developer Experience**: Writing posts feels natural—just create markdown files. The build system handles the rest, with helpful error messages when something goes wrong.\n\n**Flexibility**: I control exactly how content gets transformed. Want to add a new metadata field? Just update the types and validation logic.\n\n**Caching**: Vite's import system means content gets bundled efficiently. No duplicate processing or unnecessary re-renders.\n\n### The Trade-offs\n\n**Build Complexity**: This system adds complexity to the build process. A simple static site generator would handle this automatically.\n\n**No Hot Reload for Content**: Changing a blog post requires a full rebuild. During development, this means waiting for the content generation script to run.\n\n**Maintenance Overhead**: I have to maintain this custom code. If Vite or TypeScript changes how imports work, I need to update my system.\n\n**Learning Curve**: New contributors need to understand this custom system instead of a standard framework approach.\n\n## Alternatives I Considered\n\n**Runtime Processing**: Parse markdown in the browser or server. This would be simpler but slower, especially for large blogs.\n\n**Static Site Generators**: Use Next.js, Astro, or Eleventy. These handle content processing automatically but give less control.\n\n**Headless CMS**: Contentful or Sanity would provide a nice editing experience but add external dependencies and API calls.\n\n**File-Based CMS**: Something like Contentlayer that generates types from content. This would be similar to my approach but more opinionated.\n\n## The Result\n\nMy blog now builds content once and serves it instantly. The system catches content errors during development, provides excellent TypeScript support, and gives me complete control over the content pipeline.\n\nThe trade-off is complexity—I maintain more code than I would with a standard framework. But the performance benefits and developer experience make it worthwhile for my use case.\n\nIf you're building a blog and want maximum control over your content pipeline, this approach might work for you too. Just be prepared to maintain the build tooling alongside your content.\n\nYou can see the complete implementation in my [blog repository](https://github.com/rjleyva/rjleyva-writes). The content generation script runs automatically during builds, transforming markdown into type-safe TypeScript that my React components can import directly.\n\nWhat do you think—would you build something similar for your own blog, or stick with a more conventional approach?\n",
    "readingTime": 6
  },
  {
    "title": "Learning TypeScript Through Constraints, Not Tutorials",
    "date": "2025-12-23T00:00:00.000Z",
    "description": "How I approach learning TypeScript as a JavaScript developer by starting with strict tooling and letting constraints guide my understanding.",
    "tags": [
      "typescript",
      "tooling",
      "learning",
      "developer-experience",
      "personal-growth"
    ],
    "slug": "learning-typescript-through-constraint-not-tutorials",
    "topic": "typescript",
    "content": "\nWhen I decided to learn TypeScript, I didn’t start with a tutorial or a cheat sheet.\nI started by turning on strict mode.\n\nNot because I understood what every option did—but because I wanted the language to push back. I wanted the compiler and my tooling to surface the gaps in my understanding instead of letting them slip quietly into production.\n\nAs someone new to TypeScript but not new to JavaScript, I’ve learned that comfort is rarely a good teacher. Loose configurations let you move fast, but they also let misunderstandings hide. Strict configurations do the opposite—they interrupt you, slow you down, and force you to be explicit.\n\nSo instead of easing into TypeScript, I chose to begin with a strict tsconfig and a conservative ESLint setup. Not to be “correct” from day one, but to create a feedback loop where every warning meant something I didn’t yet understand.\n\nThis meant setting up my tooling in a way that wouldn’t let me “accidentally” write unsafe code. I wanted ESLint and TypeScript to be noisy—especially early on—so every warning became a prompt to learn, not something to ignore.\n\nHere’s an example of how I configure my `eslint.config.js` in a Vite + React + TypeScript blog.\n\n> NOTE: This isn’t a recommendation to copy this config wholesale. The value isn’t in the exact rules — it’s in choosing constraints that make incorrect assumptions impossible to ignore.\n\n```javascript\nimport jsxA11y from 'eslint-plugin-jsx-a11y'\nimport reactPlugin from 'eslint-plugin-react'\nimport reactHooks from 'eslint-plugin-react-hooks'\nimport reactRefresh from 'eslint-plugin-react-refresh'\nimport globals from 'globals'\nimport tseslint from 'typescript-eslint'\n\nexport default [\n  {\n    ignores: ['dist', 'node_modules', 'build', '.vite']\n  },\n  {\n    files: ['**/*.{ts,tsx}'],\n    languageOptions: {\n      ecmaVersion: 'latest',\n      globals: globals.browser,\n      parser: tseslint.parser,\n      parserOptions: {\n        project: ['./tsconfig.app.json', './tsconfig.node.json']\n      }\n    },\n    settings: {\n      react: {\n        version: 'detect'\n      }\n    },\n    plugins: {\n      '@typescript-eslint': tseslint.plugin,\n      react: reactPlugin,\n      'jsx-a11y': jsxA11y,\n      'react-hooks': reactHooks,\n      'react-refresh': reactRefresh\n    },\n    rules: {\n      ...reactPlugin.configs.recommended.rules,\n\n      ...jsxA11y.configs.recommended.rules,\n\n      ...reactHooks.configs.recommended.rules,\n\n      'react/react-in-jsx-scope': 'off',\n      'react/jsx-uses-react': 'off',\n      'react/prop-types': 'off',\n\n      ...tseslint.configs.strictTypeChecked[0].rules,\n      ...tseslint.configs.stylisticTypeChecked[0].rules,\n\n      '@typescript-eslint/prefer-nullish-coalescing': 'error',\n      '@typescript-eslint/prefer-optional-chain': 'error',\n      '@typescript-eslint/no-unnecessary-condition': 'error',\n      '@typescript-eslint/strict-boolean-expressions': 'error',\n      '@typescript-eslint/no-explicit-any': 'error',\n      '@typescript-eslint/no-empty-object-type': 'error',\n\n      '@typescript-eslint/no-unused-vars': [\n        'error',\n        { argsIgnorePattern: '^_', varsIgnorePattern: '^_' }\n      ],\n\n      '@typescript-eslint/explicit-function-return-type': 'error',\n      '@typescript-eslint/explicit-module-boundary-types': 'error',\n\n      'react-refresh/only-export-components': [\n        'error',\n        { allowConstantExport: true }\n      ]\n    }\n  }\n]\n```\n\nThis setup is intentionally strict. Some of these rules slowed me down at first, and that was the point. Each error forced me to ask _why_ TypeScript was unhappy—and that question is where the learning happened.\n\nAfter setting up strict linting, I carried the same philosophy into my TypeScript configuration. Rather than using a single `tsconfig` for everything, I split my setup into two: one for the browser-facing app and one for Node-based tooling.\n\nThe goal wasn’t completeness—it was clarity. Each environment should fail loudly in ways that actually matter to it.\n\nAnd this is how I setup `tsconfig.app.json`\n\n```json\n{\n  \"compilerOptions\": {\n    \"target\": \"ESNext\",\n    \"lib\": [\"ES2024\", \"DOM\", \"DOM.Iterable\"],\n    \"module\": \"ESNext\",\n    \"moduleResolution\": \"bundler\",\n    \"moduleDetection\": \"force\",\n    \"jsx\": \"react-jsx\",\n    \"useDefineForClassFields\": true,\n    \"noEmit\": true,\n    \"esModuleInterop\": true,\n    \"resolveJsonModule\": true,\n    \"allowImportingTsExtensions\": false,\n    \"isolatedModules\": true,\n    \"verbatimModuleSyntax\": true,\n    \"strict\": true,\n    \"exactOptionalPropertyTypes\": true,\n    \"noPropertyAccessFromIndexSignature\": true,\n    \"noImplicitOverride\": true,\n    \"noUncheckedIndexedAccess\": true,\n    \"forceConsistentCasingInFileNames\": true,\n    \"noUnusedLocals\": true,\n    \"noUnusedParameters\": true,\n    \"noFallthroughCasesInSwitch\": true,\n    \"skipLibCheck\": true,\n    \"baseUrl\": \"./\",\n    \"paths\": {\n      \"@/*\": [\"src/*\"]\n    }\n  },\n  \"include\": [\"src\", \"prettier.config.ts\"],\n  \"exclude\": [\"node_modules\", \"dist\"]\n}\n```\n\nTurning on `strict` was non-negotiable. It ensured that TypeScript would always assume the least about my code unless I proved otherwise.\n\n`exactOptionalPropertyTypes` forced me to be honest about my data models. Optional didn’t mean “sometimes undefined”—it meant _truly optional_.\n\nAnother excellent choice is `noUncheckedIndexedAccess`, Arrays are objects stop feeling \"safe\", this helps me learn defensive access pattern naturally.\n\n`*noPropertyAccessFromIndexSignature` subtly teaches me about structural typing, why loose onjects are dangerous and when to model with records vs interfaces.\n\n`verbatimModuleSyntax + isolatedModules` These options pushed me to write code that behaves the same way at runtime as it does in my head—no hidden transformations, no surprises.\n\nThe way I handle Node `tsconfig.node.json` is strict in the same way but scoped differently.\n\n```json\n{\n  \"compilerOptions\": {\n    \"tsBuildInfoFile\": \"./node_modules/.tmp/tsconfig.node.tsbuildinfo\",\n    \"target\": \"ESNext\",\n    \"lib\": [\"ES2024\"],\n    \"module\": \"ESNext\",\n    \"moduleResolution\": \"bundler\",\n    \"moduleDetection\": \"force\",\n    \"useDefineForClassFields\": true,\n    \"noEmit\": true,\n    \"esModuleInterop\": true,\n    \"resolveJsonModule\": true,\n    \"allowImportingTsExtensions\": false,\n    \"isolatedModules\": true,\n    \"verbatimModuleSyntax\": true,\n    \"strict\": true,\n    \"exactOptionalPropertyTypes\": true,\n    \"noPropertyAccessFromIndexSignature\": true,\n    \"noImplicitOverride\": true,\n    \"noUncheckedIndexedAccess\": true,\n    \"forceConsistentCasingInFileNames\": true,\n    \"noUnusedLocals\": true,\n    \"noUnusedParameters\": true,\n    \"noFallthroughCasesInSwitch\": true,\n    \"skipLibCheck\": true,\n    \"baseUrl\": \"./\",\n    \"paths\": {\n      \"@/*\": [\"src/*\"]\n    }\n  },\n  \"include\": [\"vite.config.ts\", \"vitest.config.ts\", \"scripts/**/*.ts\"],\n  \"exclude\": [\"node_modules\", \"dist\"]\n}\n```\n\nThe Node config mirrors most of the same constraints, but removes anything browser-specific. Keeping them aligned helps me build a consistent mental model while still respecting the environment I’m targeting.\n\nI didn’t understand all of these options upfront. Some of them only made sense _after_ they broke my assumptions. That breakage is what made them valuable.\n\nConstraints are free. The compiler doesn’t care how you learned — only whether your assumptions hold.\n\nI also came across a great extension to learn TypeScript [Total TypeScript](https://www.totaltypescript.com/vscode-extension), I'm still figuring out how use this inside Neovim, but for the time being I installed [VSCode](https://code.visualstudio.com/) to experiment with TypeScript and use it alongside [Pretty TypeScript Errors](https://open-vsx.org/extension/yoavbls/pretty-ts-errors)\n\nHere's how I use it on my own Vite + React + TypeScript [Blog](https://github.com/rjleyva/rjleyva-writes).\n",
    "readingTime": 5
  },
  {
    "title": "Implementing Dynamic Favicons",
    "date": "2025-12-25T00:00:00.000Z",
    "description": "A comprehensive exploration of building a theme-aware favicon system that adapts to both user preferences and system settings, including performance optimizations, browser compatibility considerations, and architectural trade-offs.",
    "tags": [
      "web-development",
      "accessibility",
      "performance",
      "browser-api",
      "theme-system",
      "typescript",
      "user-experience"
    ],
    "slug": "implementing-dynamic-favicons",
    "topic": "web-development",
    "content": "\nBuilding a modern web application often involves subtle details that significantly impact user experience. One such detail is the favicon—the small icon that appears in browser tabs. When I added theme switching to my blog, I realized my favicon needed to adapt too. What started as a simple enhancement evolved into a sophisticated system that handles browser preferences, theme changes, and performance optimizations.\n\nIn this post, I'll walk through how I implemented a dynamic favicon manager that automatically switches between light and dark variants based on user preferences, with careful attention to performance, accessibility, and browser compatibility.\n\n## The Problem: Static Favicons in a Dynamic World\n\nTraditional favicons are static—they don't change based on user preferences or application state. But modern web applications often support multiple themes, and users increasingly expect visual consistency across their entire browsing experience.\n\nConsider these scenarios:\n\n- A user switches their operating system to dark mode\n- A user manually toggles between light and dark themes in your application\n- The favicon becomes invisible against the browser's tab background\n- Users struggle to identify your tab among dozens of open tabs\n\nThe solution? Dynamic favicons that adapt to theme changes in real-time.\n\n## Core Architecture: The FaviconManager Class\n\nMy implementation centers around a singleton `FaviconManager` class that handles all favicon-related logic. Here's the complete implementation:\n\n```typescript\nclass FaviconManager {\n  private faviconLinkElement: HTMLLinkElement | null = null\n  private mediaQuery: MediaQueryList | null = null\n  private isInitialized: boolean = false\n\n  constructor() {}\n\n  public initialize(): void {\n    if (this.isInitialized) return\n    this.initFaviconElement()\n    this.setupMediaQueryListener()\n    this.isInitialized = true\n  }\n\n  private initFaviconElement(): void {\n    this.faviconLinkElement = document.querySelector(\n      'link[rel=\"icon\"]'\n    ) as HTMLLinkElement | null\n  }\n\n  private setupMediaQueryListener(): void {\n    this.mediaQuery = window.matchMedia('(prefers-color-scheme: dark)')\n    this.mediaQuery.addEventListener('change', () => {\n      this.updateFavicon()\n    })\n    // Set initial favicon based on current preference\n    this.updateFavicon()\n  }\n\n  public setFaviconForTheme(_isDarkTheme: boolean): void {\n    // This method is kept for backward compatibility but now uses browser preference\n    this.updateFavicon()\n  }\n\n  private updateFavicon(): void {\n    // Ensure favicon element reference is cached\n    if (!this.faviconLinkElement) {\n      this.initFaviconElement()\n    }\n\n    if (!this.faviconLinkElement) return\n\n    // Mapping logic:\n    // Use browser/system theme preference to determine favicon color for visibility\n    // against the browser tab background, not the website's theme\n    const prefersDark = window.matchMedia(\n      '(prefers-color-scheme: dark)'\n    ).matches\n    const timestamp = Date.now()\n    this.faviconLinkElement.href = prefersDark\n      ? `/favicons/favicon-light.svg?theme=dark&t=${timestamp}`\n      : `/favicons/favicon-dark.svg?theme=light&t=${timestamp}`\n  }\n}\n\nexport const faviconManager = new FaviconManager()\n\nexport const initializeFaviconManager = (): void => {\n  faviconManager.initialize()\n}\n```\n\n## Key Design Decisions and Trade-offs\n\n### Browser Preference vs. Application Theme\n\nOne of the most critical decisions was whether to base the favicon on the application's theme or the browser's system preference. I chose browser preference for several reasons:\n\n**Why Browser Preference?**\n\n- **Tab Background Consistency**: Browser tabs have their own background colors that match the system theme, not your application's theme\n- **User Expectations**: Users expect visual elements to match their system-wide theme settings\n- **Reduced Complexity**: No need to coordinate between theme state and favicon state\n\n**The Trade-off**: This means the favicon might not always match your application's theme, but it will always be visible against the tab background.\n\n### Singleton Pattern with Lazy Initialization\n\nThe `faviconManager` is exported as a singleton instance with lazy initialization:\n\n```typescript\nexport const faviconManager = new FaviconManager()\n\nexport const initializeFaviconManager = (): void => {\n  faviconManager.initialize()\n}\n```\n\n**Benefits:**\n\n- **Controlled Setup**: Initialization happens when explicitly called, providing better control\n- **Predictable State**: Single source of truth for favicon state\n- **Better Error Handling**: Initialization can be wrapped with error boundaries\n\n**Considerations:**\n\n- **Manual Initialization Required**: Must be explicitly called during app startup\n- **Global State**: Can make testing more complex\n\n### Defensive Programming and Error Handling\n\nThe implementation includes several defensive programming techniques:\n\n```typescript\nprivate updateFavicon(): void {\n  // Ensure favicon element reference is cached\n  if (!this.faviconLinkElement) {\n    this.initFaviconElement()\n  }\n\n  if (!this.faviconLinkElement) return\n  // ... rest of logic\n}\n```\n\nThis handles cases where:\n\n- The favicon link element might not exist\n- The DOM might not be ready during initialization\n- The element might be removed or modified by other scripts\n\n### Cache Busting with Timestamps\n\nThe favicon URLs include timestamps to prevent caching issues:\n\n```typescript\nconst timestamp = Date.now()\nthis.faviconLinkElement.href = prefersDark\n  ? `/favicons/favicon-light.svg?theme=dark&t=${timestamp}`\n  : `/favicons/favicon-dark.svg?theme=light&t=${timestamp}`\n```\n\nThis ensures browsers always fetch the new favicon when the theme changes, preventing stale cached versions from showing incorrect icons.\n\n## Browser API Integration\n\n### MediaQueryList and Event Listeners\n\nThe system leverages the `MediaQueryList` API to detect system theme changes:\n\n```typescript\nprivate setupMediaQueryListener(): void {\n  this.mediaQuery = window.matchMedia('(prefers-color-scheme: dark)')\n  this.mediaQuery.addEventListener('change', () => {\n    this.updateFavicon()\n  })\n  // Set initial favicon based on current preference\n  this.updateFavicon()\n}\n```\n\n**Why `addEventListener` instead of `addListener`?**\n\n- `addEventListener` is the modern standard\n- `addListener` is deprecated in favor of event listeners\n- Better consistency with other DOM APIs\n\n### Browser Compatibility Considerations\n\nThe `prefers-color-scheme` media query has excellent support:\n\n- **Chrome/Edge**: Full support since version 76\n- **Firefox**: Full support since version 67\n- **Safari**: Full support since version 12.1\n- **Mobile browsers**: Well supported across iOS Safari and Android Chrome\n\nFor older browsers, the system gracefully degrades—the favicon simply won't change with system theme preferences.\n\n## Integration with the Theme System\n\nThe favicon manager integrates seamlessly with my theme provider:\n\n```typescript\n// In ThemeProvider.tsx\nimport { faviconManager } from '@/utils/faviconManager'\n\nuseLayoutEffect(() => {\n  // ... theme setup logic\n\n  faviconManager.setFaviconForTheme(theme === THEMES.DARK)\n}, [theme])\n```\n\nInterestingly, the `setFaviconForTheme` method now ignores the parameter and uses browser preference instead. This maintains backward compatibility while implementing the improved logic.\n\n### Module Import Strategy\n\nThe favicon manager uses lazy initialization and must be explicitly initialized:\n\n```typescript\n// In main.tsx\nimport { initializeFaviconManager } from './utils/faviconManager'\n\ninitializeFaviconManager()\n```\n\nThis approach provides more control over when the favicon system initializes and allows for proper error handling during the initialization process.\n\n## Performance Optimizations\n\n### Minimal DOM Queries\n\nThe implementation caches the favicon link element to avoid repeated DOM queries:\n\n```typescript\nprivate faviconLinkElement: HTMLLinkElement | null = null\n\nprivate initFaviconElement(): void {\n  this.faviconLinkElement = document.querySelector(\n    'link[rel=\"icon\"]'\n  ) as HTMLLinkElement | null\n}\n```\n\n### Efficient Event Handling\n\nThe media query listener only triggers when the system preference actually changes, minimizing unnecessary updates.\n\n### Memory Management\n\nThe singleton pattern ensures only one instance exists, preventing memory leaks from multiple listeners or duplicate DOM references.\n\n## Alternative Approaches I Considered\n\n### CSS-Based Favicon Switching\n\nUsing CSS custom properties to dynamically change favicon colors:\n\n```html\n<link\n  rel=\"icon\"\n  href=\"data:image/svg+xml,<svg>...</svg>\"\n  type=\"image/svg+xml\"\n/>\n```\n\n**Pros:** Pure CSS solution\n**Cons:** Complex SVG encoding, limited browser support, harder to maintain\n\n### Multiple Link Elements\n\nPre-defining multiple favicon link elements and toggling their `disabled` attribute:\n\n```html\n<link\n  rel=\"icon\"\n  href=\"/favicon-light.svg\"\n  media=\"(prefers-color-scheme: light)\"\n/>\n<link\n  rel=\"icon\"\n  href=\"/favicon-dark.svg\"\n  media=\"(prefers-color-scheme: dark)\"\n/>\n```\n\n**Pros:** Declarative, no JavaScript required\n**Cons:** Limited browser support, no dynamic switching capability\n\n### Theme-Aware Asset URLs\n\nGenerating favicon URLs based on current theme state:\n\n```typescript\nconst getFaviconUrl = (theme: Theme) => `/favicons/favicon-${theme}.svg`\n```\n\n**Pros:** Simple and direct\n**Cons:** Doesn't account for system preferences, can cause visibility issues\n\n## Testing and Quality Assurance\n\n### Manual Testing Scenarios\n\nThe implementation requires testing across multiple scenarios:\n\n1. **System Theme Changes**: Switch OS theme while the app is running\n2. **Page Reloads**: Verify favicon persists across navigation\n3. **Browser Compatibility**: Test in different browsers and versions\n4. **Network Conditions**: Ensure cache busting works offline\n\n### Automated Testing Considerations\n\n```typescript\n// Example test setup\ndescribe('FaviconManager', () => {\n  let manager: FaviconManager\n  let mockLink: HTMLLinkElement\n\n  beforeEach(() => {\n    mockLink = document.createElement('link')\n    mockLink.rel = 'icon'\n    document.head.appendChild(mockLink)\n\n    manager = new FaviconManager()\n  })\n\n  it('should update favicon based on system preference', () => {\n    // Test implementation\n  })\n})\n```\n\n## Future Enhancements\n\n### User Preference Overrides\n\nAdd support for users to manually choose favicon behavior:\n\n- Always use light favicon\n- Always use dark favicon\n- Follow system preference (current behavior)\n\n### Reduced Motion Support\n\nRespect `prefers-reduced-motion` for smoother transitions:\n\n```typescript\nconst prefersReducedMotion = window.matchMedia(\n  '(prefers-reduced-motion: reduce)'\n).matches\n```\n\n### Favicon Animations\n\nFor special states (loading, notifications), consider animated favicons using canvas or CSS animations.\n\n## Performance Impact and Monitoring\n\n### Bundle Size\n\nThe favicon manager adds minimal overhead:\n\n- **Code size**: ~2KB minified\n- **Runtime memory**: Single object instance\n- **Network requests**: Only when theme changes (with cache busting)\n\n### Monitoring Setup\n\nConsider adding performance monitoring:\n\n```typescript\n// Track favicon update performance\nconst startTime = performance.now()\n// ... favicon update logic\nconst duration = performance.now() - startTime\nconsole.log(`Favicon update took ${duration}ms`)\n```\n\n## Conclusion\n\nBuilding a theme-aware favicon system required balancing user experience, performance, and technical constraints. The solution I implemented prioritizes visibility and accessibility by following browser preferences rather than application themes.\n\nKey takeaways:\n\n- **User Experience First**: Favicons should always be visible against tab backgrounds\n- **Defensive Programming**: Handle edge cases and browser inconsistencies\n- **Performance Matters**: Minimize DOM interactions and optimize for quick updates\n- **Future-Proof**: Use modern APIs while maintaining backward compatibility\n\nThe implementation demonstrates how small details can significantly impact overall user experience. While favicons might seem trivial, their proper implementation shows attention to detail that users appreciate.\n\nYou can see the complete implementation in my [blog repository](https://github.com/rjleyva/rjleyva-writes). The favicon manager runs automatically, ensuring users always have optimal visual feedback in their browser tabs.\n",
    "readingTime": 8
  },
  {
    "title": "A Journey into Full-Stack Development: Basic Monorepo Setup",
    "date": "2025-12-29T00:00:00.000Z",
    "description": "Join me on my journey into full-stack development as I set up a basic monorepo for *Yaru Koto* (やること – “things to do”). Using **pnpm workspaces**, I navigated the challenges of managing frontend and backend code, sharing TypeScript types, and orchestrating builds across packages. This post covers the monorepo setup, shared package architecture, and lessons learned along the way—perfect for solo developers or anyone curious about organizing a full-stack project efficiently.",
    "tags": [
      "pnpm",
      "monorepo",
      "typescript",
      "react",
      "vite",
      "nodejs",
      "express",
      "full-stack",
      "web-development"
    ],
    "slug": "pnpm-mono-repo-setup",
    "topic": "web-development",
    "content": "\nSo I decided I wanted to learn full-stack development. You know, actually build something end-to-end instead of just following tutorials. But I quickly realized that managing separate repos for frontend and backend sounded like a total nightmare - keeping types in sync, dealing with version mismatches, all that jazz.\n\nAfter spending way too many late nights reading blog posts and docs (and getting confused by Nx vs. Lerna vs. Turborepo), I settled on pnpm workspaces. It felt like the sweet spot - powerful enough to handle what I needed, but not overkill for a solo developer just trying to figure things out.\n\nThis is the story of how I set up \"Yaru Koto\" (やること - \"things to do\" in Japanese), my little todo app that's basically my full-stack playground. Spoiler: it took way longer than I expected, but I'm kinda proud of it now.\n\n## Why a Monorepo? (Or: Why I Didn't Just Make Two Separate Repos)\n\nI know what you're thinking - \"Why not just put the frontend and backend in separate repos like a normal person?\" Well, let me tell you about the nightmare I was trying to avoid:\n\n1. **Shared Code Hell**: I kept imagining myself copy-pasting TypeScript types between projects and then forgetting to update one side. Been there, done that, got the merge conflicts.\n\n2. **Dependency Drama**: Managing versions across two repos sounded exhausting. What if my frontend is on React 18 and backend is still on 17? Chaos.\n\n3. **Learning Excuse**: Honestly, I wanted to learn how big projects organize code. Plus it sounded impressive to say I was working in a monorepo.\n\n4. **\"What if it gets big?\"**: I figured if this todo app actually takes off (lol), I won't have to do a massive refactor later.\n\n## Project Structure Overview\n\n```\nyaru-koto/\n├── client/          # React frontend\n├── server/          # Express.js backend\n├── shared/          # Shared packages\n│   ├── constants/   # Application constants\n│   ├── types/       # TypeScript type definitions\n│   └── utils/       # Utility functions\n├── package.json     # Root package configuration\n└── pnpm-workspace.yaml # Workspace configuration\n```\n\n## Setting Up the Workspace Configuration\n\nThis part took me way longer than it should have. I kept reading docs and seeing different examples, and I was like \"Why can't anyone just show me the exact file I need?\"\n\nThe foundation of any pnpm monorepo is the `pnpm-workspace.yaml` file. Here's what mine ended up looking like after a few failed attempts:\n\n```yaml\npackages:\n  - client\n  - server\n  - shared/*\n\nonlyBuiltDependencies:\n  - '@swc/core'\n  - esbuild\n  - sqlite3\n```\n\nI spent a good hour trying different variations before realizing that `packages` just needs to list the directories containing your packages. The `shared/*` part was my \"aha!\" moment - it automatically picks up all the packages in that folder without listing them individually.\n\nThe `onlyBuiltDependencies` section? Yeah, I copy-pasted that from somewhere and it just worked. Something about native modules needing to be built first. Honestly, I don't fully understand it yet, but I'm glad it exists.\n\n## Root Package.json Configuration\n\nI probably spent the most time on this root package.json. I kept adding scripts, removing them, trying different approaches. It's basically my command center now.\n\n```json\n{\n  \"name\": \"yaru-koto\",\n  \"author\": \"RJ Leyva\",\n  \"description\": \"Yaru Koto (やること) means \\\"things to do\\\" in Japanese...\",\n  \"scripts\": {\n    \"dev\": \"pnpm --filter client dev\",\n    \"dev:server\": \"pnpm --filter server dev\",\n    \"dev:all\": \"pnpm -r dev\",\n    \"build\": \"pnpm --filter client build\",\n    \"lint\": \"pnpm --filter client lint\"\n  }\n}\n```\n\nThe magic here is pnpm's `--filter` flag. I was so confused at first - why not just run the scripts directly? But then I got it:\n\n- `pnpm --filter client dev` = \"run the dev script, but only for the client package\"\n- `pnpm -r dev` = \"run dev scripts for ALL packages\" (the `-r` means recursive)\n\nI love how `pnpm dev:all` starts my whole stack. No more remembering which terminal window has what.\n\n## Shared Packages: The Heart of the Monorepo\n\nThis is where things got really interesting (and frustrating). One of the biggest advantages of a monorepo is sharing code between packages. I created three shared packages, but figuring out how they should depend on each other was a brain-twister:\n\n### 1. Types Package (`@yaru-koto/types`)\n\nThis package contains all TypeScript type definitions used across the application:\n\n```json\n{\n  \"name\": \"@yaru-koto/types\",\n  \"main\": \"dist/index.js\",\n  \"types\": \"dist/index.d.ts\",\n  \"scripts\": {\n    \"build\": \"tsc\",\n    \"dev\": \"tsc --watch\"\n  }\n}\n```\n\nThe TypeScript configuration compiles to a `dist/` directory with both JavaScript and declaration files.\n\n### 2. Constants Package (`@yaru-koto/constants`)\n\nThis package contains all application constants, including todo priorities, statuses, API endpoints, and default values:\n\n```json\n{\n  \"name\": \"@yaru-koto/constants\",\n  \"main\": \"dist/index.js\",\n  \"types\": \"dist/index.d.ts\",\n  \"scripts\": {\n    \"build\": \"tsc\",\n    \"dev\": \"tsc --watch\"\n  }\n}\n```\n\n### 3. Utils Package (`@yaru-koto/utils`)\n\nThis one was the trickiest. Common utility functions go here, but some of my utilities needed to use the types from the types package. I stared at this for like 20 minutes wondering if this was even allowed.\n\n```json\n{\n  \"name\": \"@yaru-koto/utils\",\n  \"main\": \"dist/index.js\",\n  \"types\": \"dist/index.d.ts\",\n  \"dependencies\": {\n    \"@yaru-koto/types\": \"workspace:*\"\n  },\n  \"scripts\": {\n    \"build\": \"tsc\",\n    \"dev\": \"tsc --watch\"\n  }\n}\n```\n\nThe `\"@yaru-koto/types\": \"workspace:*\"` line was my \"oh wait, this actually works?\" moment. It means \"use whatever version of types is in this workspace\". Super simple once I figured it out, but I was overcomplicating it in my head.\n\n## Client Configuration: React + Vite\n\nThe frontend is a modern React application using Vite for fast development:\n\n```json\n{\n  \"name\": \"@yaru-koto/client\",\n  \"dependencies\": {\n    \"react\": \"^19.2.3\",\n    \"react-dom\": \"^19.2.3\",\n    \"@yaru-koto/types\": \"workspace:*\",\n    \"@yaru-koto/constants\": \"workspace:*\",\n    \"@yaru-koto/utils\": \"workspace:*\"\n  },\n  \"devDependencies\": {\n    \"@vitejs/plugin-react-swc\": \"^4.2.2\",\n    \"typescript\": \"~5.9.3\",\n    \"vite\": \"^7.3.0\"\n  }\n}\n```\n\nKey points:\n\n- Uses `workspace:*` to reference shared packages\n- SWC for fast React compilation\n- TypeScript for type safety\n\n## Server Configuration: Express + SQLite3\n\nThe backend is a simple Express.js server with SQLite3:\n\n```json\n{\n  \"name\": \"@yaru-koto/server\",\n  \"dependencies\": {\n    \"express\": \"^5.2.1\",\n    \"sqlite3\": \"^5.1.7\",\n    \"@yaru-koto/types\": \"workspace:*\",\n    \"@yaru-koto/constants\": \"workspace:*\",\n    \"@yaru-koto/utils\": \"workspace:*\"\n  },\n  \"devDependencies\": {\n    \"nodemon\": \"^3.1.11\",\n    \"ts-node\": \"^10.9.2\",\n    \"typescript\": \"~5.9.3\"\n  }\n}\n```\n\nThe server uses nodemon with ts-node for development, allowing hot-reloading during development.\n\n## TypeScript Configuration Across Packages\n\nEach package has its own `tsconfig.json`, but they share similar configurations:\n\n```json\n{\n  \"compilerOptions\": {\n    \"target\": \"ES2020\",\n    \"module\": \"commonjs\",\n    \"declaration\": true,\n    \"outDir\": \"./dist\",\n    \"rootDir\": \"./src\",\n    \"strict\": true,\n    \"esModuleInterop\": true,\n    \"skipLibCheck\": true,\n    \"forceConsistentCasingInFileNames\": true\n  }\n}\n```\n\nThe shared packages output both JavaScript and TypeScript declaration files to their `dist/` directories.\n\n## Development Workflow\n\nNow that everything's set up, my development workflow is actually pretty smooth. I can:\n\n1. **Start the entire stack**: `pnpm dev:all` (my favorite - opens everything at once)\n2. **Work on frontend only**: `pnpm dev` (when I'm just tweaking the UI)\n3. **Work on backend only**: `pnpm dev:server` (for API testing)\n4. **Build for production**: `pnpm build` (usually works on the first try now!)\n5. **Run linting**: `pnpm lint` (catches my dumb mistakes)\n\n## Challenges and Learnings (Or: The Parts That Made Me Want to Quit)\n\nSetting up this monorepo wasn't the smooth sailing I expected. Here are the things that actually made me sweat:\n\n1. **Inter-package Dependencies**: I spent a full evening trying to figure out why my utils package couldn't import from types. Turns out I needed to add the dependency in package.json AND run `pnpm install`. Every. Single. Time. I made a change.\n\n2. **Build Order Drama**: I kept getting \"Cannot find module\" errors because I was trying to use packages before they were built. The shared packages need to be built before the client/server can use them. I learned this the hard way after about 5 failed builds.\n\n3. **Workspace References**: I was so confused about `workspace:*` vs `\"1.0.0\"` vs `\"^1.0.0\"`. Why would you ever NOT use workspace:\\* inside a monorepo? It took me forever to understand this was for when you publish packages externally.\n\n4. **TypeScript Path Resolution**: My IDE was happy, but `tsc` kept complaining it couldn't find the shared packages. I eventually realized I needed to build the shared packages first. Every. Single. Time. I made a change. (Did I mention that already?)\n\n## What I Learned\n\nThis monorepo setup taught me several important concepts:\n\n1. **Package Management**: Deep understanding of how pnpm manages workspace dependencies\n2. **Shared Package Architecture**: How to structure packages that depend on each other within a monorepo\n3. **Build Pipelines**: The importance of proper build ordering when packages have inter-dependencies\n4. **Code Organization**: How to structure large applications with clear separation of concerns\n\n## Future Improvements (When I Get Around to It)\n\nOnce I actually finish building the core todo functionality, I want to add:\n\n1. **Automated Testing**: I know, I know - I should have done this first. Jest or Vitest, probably.\n2. **CI/CD Pipeline**: GitHub Actions to automatically test and deploy. Sounds fancy!\n3. **Docker Setup**: So I can actually deploy this thing somewhere\n4. **More Shared Packages**: I'm sure I'll find more code to extract as I build features\n\nBut first, let me make sure users can actually create todos. The monorepo can wait.\n\n## Conclusion\n\nWow, what a journey. Setting up this monorepo kicked my butt in ways I didn't expect. I thought I'd just follow some docs and boom - organized codebase! Instead, I learned more about package management, build processes, and TypeScript than I ever wanted to know.\n\nBut you know what? I'm actually kinda proud of myself. I now understand how big projects stay organized, and I can confidently talk about monorepos at meetups (not that anyone would ask, but still).\n\nThe pnpm workspace approach was definitely the right call for a learning project like this. It's powerful enough to handle real complexity, but simple enough that I didn't drown in configuration.\n\nNow that I have this foundation, I can't wait to actually build features for Yaru Koto instead of fighting with the build system. Next up: making this todo app actually work!\n",
    "readingTime": 9
  },
  {
    "title": "Security-First Markdown Processing",
    "date": "2025-12-27T00:00:00.000Z",
    "description": "A deep dive into my restrictive markdown sanitization approach that prioritizes security while maintaining rich content features, exploring the trade-offs and implementation details.",
    "tags": [
      "security",
      "markdown",
      "web-development",
      "sanitization",
      "content-security",
      "unified",
      "rehype"
    ],
    "slug": "security-first-markdown-processing",
    "topic": "web-development",
    "content": "\n# Security-First Markdown Processing\n\nWhen I built my blog's markdown processing pipeline, I made a conscious decision to prioritize security over convenience. Most markdown processors take a permissive approach—allowing HTML and trusting the content source. I went the opposite direction: deny everything by default, allow only what's explicitly safe.\n\nThis approach isn't just paranoia. It's a response to real security threats in web content processing. Let me walk you through how I implemented this security-first markdown pipeline and why I made the choices I did.\n\n## The Security Threat Landscape\n\nBefore diving into the implementation, it's worth understanding why markdown processing can be dangerous. Markdown itself is safe—it's just text with formatting syntax. But when you convert markdown to HTML, you open potential attack vectors:\n\n**Cross-Site Scripting (XSS)**: Malicious HTML or JavaScript injected into content\n**Data Exfiltration**: Scripts that steal user data or session information\n**Content Injection**: Unauthorized content that manipulates page appearance or behavior\n**Supply Chain Attacks**: Compromised dependencies in the processing pipeline\n\nEven if you trust your content sources (I write all my own posts), security best practices recommend defense in depth. A single vulnerability in any part of your system could compromise everything.\n\n## The Deny-All, Allow-Specific Model\n\nMy approach follows the principle of least privilege: deny everything by default, allow only what's explicitly permitted. Here's how this manifests in my `rehype-sanitize` configuration:\n\n```typescript\n{\n  // Allow only safe HTML elements and attributes\n  allowDoctypes: false,\n  allowComments: false,\n\n  // Strip dangerous attributes - be very restrictive\n  attributes: {\n    '*': ['className', 'id', 'lang'],\n    a: ['href', 'title', 'rel'],\n    code: ['className'],\n    pre: ['className'],\n    h1: ['id'],\n    h2: ['id'],\n    h3: ['id'],\n    h4: ['id'],\n    h5: ['id'],\n    h6: ['id']\n  },\n\n  // Allow only safe elements (no images, iframes, etc.)\n  tagNames: [\n    'p', 'div', 'span', 'br', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6',\n    'ul', 'ol', 'li', 'blockquote', 'pre', 'code', 'strong', 'em',\n    'del', 'a', 'hr', 'table', 'thead', 'tbody', 'tr', 'th', 'td'\n  ],\n\n  // Strip all protocols except safe ones\n  protocols: {\n    href: ['http', 'https', 'mailto']\n  },\n\n  // Strip any content that looks like HTML with dangerous attributes\n  strip: [\n    'script', 'style', 'iframe', 'object', 'embed', 'form', 'input',\n    'img'\n  ]\n}\n```\n\nThis configuration is aggressively restrictive. Let me break down the key decisions:\n\n### Element Whitelist\n\nI only allow elements that are essential for rich text content:\n\n- **Text formatting**: `p`, `strong`, `em`, `del`, `br`\n- **Structure**: `h1`-`h6`, `div`, `span`\n- **Lists**: `ul`, `ol`, `li`\n- **Links**: `a` (with restricted attributes)\n- **Code**: `pre`, `code` (crucial for technical content)\n- **Tables**: `table`, `thead`, `tbody`, `tr`, `th`, `td`\n- **Miscellaneous**: `blockquote`, `hr`\n\nNotice what's missing: `img`, `iframe`, `script`, `style`, `form`, `input`, `object`, `embed`. These elements are common attack vectors.\n\n### Attribute Restrictions\n\nAttributes are even more restricted than elements:\n\n```typescript\nattributes: {\n  '*': ['className', 'id', 'lang'],  // Global attributes\n  a: ['href', 'title', 'rel'],       // Link-specific\n  code: ['className'],               // Code highlighting\n  pre: ['className'],                // Code block styling\n  'h1,h2,h3,h4,h5,h6': ['id']        // Anchor links\n}\n```\n\nNo `onclick`, `onload`, `style`, `data-*`, or other potentially dangerous attributes. Even CSS classes are restricted to what I explicitly allow in my styling system.\n\n### Protocol Filtering\n\nFor links, I only allow safe protocols:\n\n```typescript\nprotocols: {\n  href: ['http', 'https', 'mailto']\n}\n```\n\nThis prevents `javascript:`, `data:`, `vbscript:`, and other executable protocol attacks.\n\n## Why This Approach vs. Permissive Sanitization\n\nMost markdown processors use more permissive sanitization. Libraries like `DOMPurify` or `sanitize-html` allow broader HTML support and rely on pattern matching to remove dangerous content. I chose the opposite approach for several reasons:\n\n### Predictability\n\nWith a whitelist approach, I know exactly what can appear in my content. There's no risk of missing a new attack pattern or edge case. If an element or attribute isn't in my list, it gets stripped—period.\n\n### Performance\n\nWhitelist-based sanitization is faster than pattern-matching approaches. There's no complex regex or DOM manipulation—just a simple lookup table.\n\n### Maintenance\n\nA restrictive whitelist requires less maintenance than trying to keep up with new attack vectors. I don't need to constantly update patterns or worry about bypass techniques.\n\n### Philosophy\n\nThis aligns with my \"secure by default\" philosophy. I'd rather content authors (me) work within constraints than risk security through complexity.\n\n## Real-World Security Considerations\n\nImplementing this security model required thinking about practical implications:\n\n### Content Author Experience\n\nThe restrictive approach means I can't use certain markdown features:\n\n- No images (they get stripped)\n- No embedded content (YouTube, Twitter, etc.)\n- No custom HTML classes beyond what I define\n- No inline styles or complex formatting\n\nFor my use case, this is acceptable. I focus on text content with code examples. If I need images, I handle them outside the markdown pipeline.\n\n### Attack Surface Reduction\n\nBy stripping dangerous elements, I eliminate entire classes of attacks:\n\n- **No script execution**: `script`, `iframe`, `object` elements removed\n- **No style injection**: `style` elements and `style` attributes stripped\n- **No form manipulation**: `form`, `input` elements removed\n- **No data exfiltration**: No way to inject tracking pixels or data URLs\n\n### Dependency Security\n\nI use the unified ecosystem (`remark`, `rehype`, `rehype-sanitize`) which has good security track records. But I still pin versions and monitor for updates, treating the processing pipeline as critical infrastructure.\n\n## Balancing Security with Rich Content\n\nThe biggest challenge is maintaining rich content features while staying secure. Here's how I handle this:\n\n### Code Block Handling\n\nCode blocks are crucial for technical writing, but `pre` and `code` elements can be security risks if not handled properly. My solution:\n\n```typescript\nconst createPreComponent = ({\n  children,\n  className\n}: {\n  children?: React.ReactNode\n  className?: string\n}): React.ReactElement => {\n  return CodeBlock({ children, className })\n}\n```\n\nI replace HTML `pre` elements with my custom `CodeBlock` React component during the `rehype-react` phase. This gives me complete control over how code renders and allows me to add copy-to-clipboard functionality.\n\nThe sanitization allows `className` attributes on `pre` and `code` elements, so language hints (`language-javascript`) work properly for potential future enhancements.\n\n### Link Security\n\nLinks are powerful but dangerous. My configuration allows `href`, `title`, and `rel` attributes:\n\n```typescript\na: ['href', 'title', 'rel']\n```\n\nI could add `noopener` and `noreferrer` to the `rel` attribute for external links, but currently handle this in my link components. The protocol filtering prevents malicious links.\n\n### Table Support\n\nTables are useful for technical content but can be complex. I allow the basic table elements (`table`, `thead`, `tbody`, `tr`, `th`, `td`) with minimal attributes. This provides rich formatting without excessive complexity.\n\n## Implementation in the Unified Pipeline\n\nHere's how this fits into my processing pipeline:\n\n```typescript\nconst processor = unified()\n  .use(remarkParse)\n  .use(remarkGfm) // GitHub Flavored Markdown\n  .use(remarkFrontmatter)\n  .use(remarkRehype, {\n    allowDangerousHtml: false // Never allow raw HTML\n  })\n  .use(rehypeSanitize, {\n    // The restrictive configuration above\n  })\n  .use(rehypeSlug) // Add IDs to headings\n  .use(rehypeReact, {\n    Fragment: jsxRuntime.Fragment,\n    jsx: jsxRuntime.jsx,\n    jsxs: jsxRuntime.jsxs,\n    components: {\n      pre: createPreComponent // Custom code block handling\n    }\n  })\n```\n\nThe `allowDangerousHtml: false` ensures remark never passes raw HTML to rehype, and the sanitization layer catches any edge cases.\n\n## Trade-offs and Limitations\n\nThis approach isn't perfect:\n\n**Content Restrictions**: I can't embed rich media or use complex HTML\n**Maintenance**: I must update the whitelist if I need new features\n**False Positives**: Legitimate content might get stripped if it uses unexpected attributes\n**Complexity**: More complex than permissive approaches\n\nBut the security benefits outweigh these costs for my use case.\n\n## Alternatives I Considered\n\n**DOMPurify**: Excellent library, but relies on pattern matching rather than whitelisting\n**Trusted Types**: Browser API for sanitization, but not available everywhere\n**CSP**: Content Security Policy helps, but doesn't replace input sanitization\n**Runtime Validation**: Checking content after processing, but misses the defense-in-depth approach\n\n## The Result\n\nMy blog processes markdown with military-grade paranoia. Every piece of content gets stripped of potential threats before reaching the browser. This gives me confidence that my content is safe, even if dependencies get compromised or unexpected input appears.\n\nThe approach scales well and requires minimal maintenance. When I need new features, I extend the whitelist deliberately rather than opening security holes.\n\nIf you're building a content system that prioritizes security—especially for user-generated content—this deny-all, allow-specific approach is worth considering. It might feel restrictive at first, but the peace of mind is invaluable.\n\nYou can see the complete implementation in my [blog repository](https://github.com/rjleyva/rjleyva-writes). The security configuration lives in `src/lib/markdownRender.ts`, where the sanitization rules are defined alongside the unified processing pipeline.\n",
    "readingTime": 7
  },
  {
    "title": "My WezTerm Terminal Setup",
    "date": "2025-12-05T00:00:00.000Z",
    "description": "How I tuned my terminal for long, focused coding sessions by reducing visual noise and prioritizing ergonomics.",
    "tags": [
      "wezterm",
      "terminal",
      "productivity",
      "setup"
    ],
    "slug": "wezterm-terminal-setup",
    "topic": "wezterm",
    "content": "\n# Finding Zen in My Terminal\n\nI spend most of my day in the terminal. At first, I didn’t notice the small UI\ndetails that quietly distracted me—tab bars, window chrome, slight background\nclutter. Over time, I realized these little things were adding noise and making\nlong coding sessions more tiring than they needed to be.\n\nThis is the story of how I stripped my terminal down to just what I need,\ncreating a setup that lets me focus for hours without friction.\n\n> This isn’t a “best practices” guide. It’s just what works for me.\n\n## Saying Goodbye to the Tab Bar\n\nOnce I leaned on TMUX for splits and layouts, the tab bar became redundant. I\nturned it off:\n\n```lua\nenable_tab_bar = false\n```\n\nIt was like decluttering my desk. Suddenly, there was nothing in my way but the\ntext.\n\n## Minimal but Practical\n\nI also wanted to keep the window frame minimal but practical. I don’t need full\nwindow chrome, just enough to resize the terminal when I need it:\n\n```lua\nwindow_decorations = \"RESIZE\"\n```\n\n## A Subtle Blur for Separation (macOS)\n\nOn macOS, I added a subtle blur behind the terminal. I experimented with\ntransparency and different blur levels until I landed on something gentle:\n\n```lua\nmacos_window_background_blur = 10\n```\n\nIt gives a slight separation from the desktop without making the text fuzzy—just\nenough breathing room to reduce eye fatigue.\n\n## Full Opacity for Comfort\n\nI used to play with transparency, but in the end, full opacity won. The text\nfeels more consistent and easier on the eyes:\n\n```lua\nwindow_background_opacity = 1.0\n```\n\n## Choosing a Font That Works\n\nFonts are surprisingly important. I went with Lilex Nerd Font because it’s\nreadable and has great glyph coverage. I also bumped the size up—comfort over\ncramming as many lines as possible:\n\n```lua\nfont = wezterm.font_with_fallback({ \"Lilex Nerd Font\" })\nfont_size = 18\n```\n\n## Scrollback That Actually Works\n\nScrollback is another small tweak that makes a big difference. A deeper buffer\nmeans I can review long logs or retrace commands without frustration:\n\n```lua\nscrollback_lines = 10000\n```\n\n## Colors That Don’t Tire Your Eyes\n\nFinally, colors. I use the solarized-osaka palette because it’s easy on the eyes\nfor long sessions. Nothing flashy, nothing harsh:\n\n```lua\ncolors = {\n  foreground = \"#839395\",\n  background = \"#001419\",\n\n  cursor_bg = \"#839395\",\n  cursor_border = \"#839395\",\n  cursor_fg = \"#001419\",\n\n  selection_bg = \"#1a6397\",\n  selection_fg = \"#839395\",\n\n  ansi = {\n    \"#001014\",\n    \"#db302d\",\n    \"#849900\",\n    \"#b28500\",\n    \"#268bd3\",\n    \"#d23681\",\n    \"#29a298\",\n    \"#9eabac\",\n  },\n\n  brights = {\n    \"#001419\",\n    \"#db302d\",\n    \"#849900\",\n    \"#b28500\",\n    \"#268bd3\",\n    \"#d23681\",\n    \"#29a298\",\n    \"#839395\",\n  },\n},\n```\n\n## The Full Zen Configuration\n\n```lua\nlocal wezterm = require(\"wezterm\")\n\nlocal M = {}\n\nM.spec = {\n  enable_tab_bar = false,\n  window_decorations = \"RESIZE\",\n  window_background_opacity = 1.0,\n  macos_window_background_blur = 10,\n  font = wezterm.font_with_fallback({ \"Lilex Nerd Font\" }),\n  font_size = 18,\n  scrollback_lines = 10000,\n\n  colors = {\n    foreground = \"#839395\",\n    background = \"#001419\",\n\n    cursor_bg = \"#839395\",\n    cursor_border = \"#839395\",\n    cursor_fg = \"#001419\",\n\n    selection_bg = \"#1a6397\",\n    selection_fg = \"#839395\",\n\n    ansi = {\n      \"#001014\",\n      \"#db302d\",\n      \"#849900\",\n      \"#b28500\",\n      \"#268bd3\",\n      \"#d23681\",\n      \"#29a298\",\n      \"#9eabac\",\n    },\n\n    brights = {\n      \"#001419\",\n      \"#db302d\",\n      \"#849900\",\n      \"#b28500\",\n      \"#268bd3\",\n      \"#d23681\",\n      \"#29a298\",\n      \"#839395\",\n    },\n  },\n}\n\nreturn M.spec\n```\n\nOver time, I realized that small adjustments—fonts, colors, scrollback, even\nsubtle blur—compound in impact. Each tweak reduces friction, letting the terminal\nfade into the background so I can focus on code.\n\nThis setup is my “zen mode.” It’s not perfect and it will probably evolve, but\nfor now, it transforms the terminal from a tool I wrestle with into a space I can\ninhabit for hours, fully immersed in work.\n",
    "readingTime": 3
  }
] as const
